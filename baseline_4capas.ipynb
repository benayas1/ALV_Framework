{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "baseline-4capas.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benayas1/ALV_Framework/blob/master/baseline_4capas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBuctOYrbfew",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "760fbd7b-dc6e-45da-e61b-1b488a11e15a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOqSYuOJbdG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#the basics\n",
        "import pandas as pd, numpy as np\n",
        "import math, json, gc, random, os, sys\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "#tensorflow deep learning basics\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow.keras.layers as L\n",
        "\n",
        "import time\n",
        "\n",
        "#for model evaluation\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "ROOT = '/content/drive/My Drive/Kaggle/covid/'\n",
        "PATH = 'baseline_4layers/'"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7g9rZfMbY2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Seed\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    \n",
        "SEED=42\n",
        "seed_everything(SEED)\n",
        "\n",
        "def adjust(public_df, private_df, public_pred, private_pred):\n",
        "    predictions = []\n",
        "\n",
        "    for df, preds in [(public_df, public_pred), (private_df, private_pred)]:\n",
        "        for i, uid in enumerate(df.id):\n",
        "            single_pred = preds[i]\n",
        "\n",
        "            single_df = pd.DataFrame(single_pred, columns=target_cols)\n",
        "            single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n",
        "\n",
        "            predictions.append(single_df)\n",
        "\n",
        "    preds_df = pd.concat(predictions)\n",
        "    return preds_df\n",
        "    \n",
        "def blend_column(dfs, weights, column):\n",
        "    values = np.array([df[column]*w for df, w in list(zip(dfs, weights))])\n",
        "    values = np.sum(values, axis=0)\n",
        "    return values\n",
        "\n",
        "def get_fold(fold, folds):\n",
        "    train_idx = np.concatenate([folds[i] for i in range(len(folds)) if i != fold])\n",
        "    val_idx = folds[fold]\n",
        "    return train_idx, val_idx\n",
        "\n",
        "def preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type'], map_tokens=None):\n",
        "    return np.transpose(\n",
        "        np.array(\n",
        "            df[cols]\n",
        "            .applymap(lambda seq: [map_tokens[x] for x in seq])\n",
        "            .values\n",
        "            .tolist()\n",
        "        ),\n",
        "        (0, 2, 1)\n",
        "    )\n",
        "\n",
        "def load_predict(model, path, public_inputs, private_inputs):\n",
        "    short = build_model(model, seq_len=107, pred_len=107)\n",
        "    short.load_weights(path)\n",
        "    long =  build_model(model, seq_len=130, pred_len=130)\n",
        "    long.load_weights(path)\n",
        "    public = short.predict(public_inputs)\n",
        "    private = long.predict(private_inputs)\n",
        "    return public, private\n",
        "\n",
        "def MCRMSE(y_true, y_pred):\n",
        "    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n",
        "    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\n",
        "\n",
        "def pairs(seq, structure):\n",
        "    pairs = seq.copy()\n",
        "    stack = []\n",
        "    for i,s in enumerate(structure):\n",
        "        if s == '(':\n",
        "            stack.append((i,seq[i]))\n",
        "        if s == ')':\n",
        "            pos, pair = stack.pop()\n",
        "            pairs[pos] = seq[i]\n",
        "            pairs[i] = pair\n",
        "    return pairs"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yky5E3JbY3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get comp data\n",
        "train = pd.read_json(ROOT+'train.json', lines=True)\n",
        "test = pd.read_json(ROOT+'test.json', lines=True)\n",
        "sample_sub = pd.read_csv(ROOT+'sample_submission.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6-YUNqwbY3T",
        "colab_type": "text"
      },
      "source": [
        "# Brief EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHlrYJ1DbY3V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "e97fd2da-ce0b-4111-d183-9aec1282d7c9"
      },
      "source": [
        "train['pairs'] = train.apply(lambda x: pairs(list(x['sequence']), list(x['structure'])), axis=1)\n",
        "test['pairs'] = test.apply(lambda x: pairs(list(x['sequence']), list(x['structure'])), axis=1)\n",
        "print(train.shape)\n",
        "if ~ train.isnull().values.any(): print('No missing values')\n",
        "print(test.shape)\n",
        "if ~ test.isnull().values.any(): print('No missing values')\n",
        "print(sample_sub.shape)\n",
        "if ~ sample_sub.isnull().values.any(): print('No missing values')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2400, 20)\n",
            "No missing values\n",
            "(3634, 8)\n",
            "No missing values\n",
            "(457953, 6)\n",
            "No missing values\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhzYyobObY3h",
        "colab_type": "text"
      },
      "source": [
        "# Processing\n",
        "\n",
        "**From the data [description tab](https://www.kaggle.com/c/stanford-covid-vaccine/data), we must predict multiple ground truths in this competition, 5 to be exact. While the submission requires all 5, only 3 are scored: `reactivity`, `deg_Mg_pH10` and `deg_Mg_50C`**\n",
        "\n",
        "**The training features we are given are as follows:**\n",
        "\n",
        "* **id** - An arbitrary identifier for each sample.\n",
        "* **seq_scored** - (68 in Train and Public Test, 91 in Private Test) Integer value denoting the number of positions used in scoring with predicted values. This should match the length of `reactivity`, `deg_*` and `*_error_*` columns. Note that molecules used for the Private Test will be longer than those in the Train and Public Test data, so the size of this vector will be different.\n",
        "* **seq_length** - (107 in Train and Public Test, 130 in Private Test) Integer values, denotes the length of `sequence`. Note that molecules used for the Private Test will be longer than those in the Train and Public Test data, so the size of this vector will be different.\n",
        "* **sequence** - (1x107 string in Train and Public Test, 130 in Private Test) Describes the RNA sequence, a combination of `A`, `G`, `U`, and `C` for each sample. Should be 107 characters long, and the first 68 bases should correspond to the 68 positions specified in `seq_scored` (note: indexed starting at 0).\n",
        "* **structure** - (1x107 string in Train and Public Test, 130 in Private Test) An array of `(`, `)`, and `.` characters that describe whether a base is estimated to be paired or unpaired. Paired bases are denoted by opening and closing parentheses e.g. (....) means that base 0 is paired to base 5, and bases 1-4 are unpaired.\n",
        "* **reactivity** - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as `seq_scored`. These numbers are reactivity values for the first 68 bases as denoted in `sequence`, and used to determine the likely secondary structure of the RNA sample.\n",
        "* **deg_pH10** - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as `seq_scored`. These numbers are reactivity values for the first 68 bases as denoted in `sequence`, and used to determine the likelihood of degradation at the base/linkage after incubating without magnesium at high pH (pH 10).\n",
        "* **deg_Mg_pH10** - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as `seq_scored`. These numbers are reactivity values for the first 68 bases as denoted in `sequence`, and used to determine the likelihood of degradation at the base/linkage after incubating with magnesium in high pH (pH 10).\n",
        "* **deg_50C** - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as `seq_scored`. These numbers are reactivity values for the first 68 bases as denoted in `sequence`, and used to determine the likelihood of degradation at the base/linkage after incubating without magnesium at high temperature (50 degrees Celsius).\n",
        "* **deg_Mg_50C** - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as `seq_scored`. These numbers are reactivity values for the first 68 bases as denoted in `sequence`, and used to determine the likelihood of degradation at the base/linkage after incubating with magnesium at high temperature (50 degrees Celsius).\n",
        "* **`*_error_*`** - An array of floating point numbers, should have the same length as the corresponding `reactivity` or `deg_*` columns, calculated errors in experimental values obtained in `reactivity` and `deg_*` columns.\n",
        "* **predicted_loop_type** - (1x107 string) Describes the structural context (also referred to as 'loop type')of each character in `sequence`. Loop types assigned by bpRNA from Vienna RNAfold 2 structure. From the bpRNA_documentation: S: paired \"Stem\" M: Multiloop I: Internal loop B: Bulge H: Hairpin loop E: dangling End X: eXternal loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS-_L3XjbY3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#target columns\n",
        "target_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adGlNayKbY3q",
        "colab_type": "text"
      },
      "source": [
        "**Now we create a dictionary to help us map `sequence`, `structure`, and `predicted_loop_type` to columns we can feed a model:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qb8iCOZbY3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mJqOkI0bY3x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2217f742-252d-49d4-bbe1-cc6c2ec8c480"
      },
      "source": [
        "train = train[train.signal_to_noise > 1]  # some signals have a lot of noise\n",
        "train_inputs = preprocess_inputs(train, cols=['sequence', 'structure', 'predicted_loop_type', 'pairs'], map_tokens=token2int)\n",
        "train_labels = np.array(train[target_cols].values.tolist()).transpose((0, 2, 1))\n",
        "print('Train Input shape', train_inputs.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Input shape (2096, 107, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ESpMT7MbY32",
        "colab_type": "text"
      },
      "source": [
        "# Model\n",
        "\n",
        "**We begin with a simple GRU model taken from the one and only [Xhlulu](https://www.kaggle.com/xhlulu)'s notebook [here](https://www.kaggle.com/xhlulu/openvaccine-simple-gru-model)**\n",
        "\n",
        "**From the documentation of this competition, you can read that due to technical reasons, measurements cannot be carried out on the final bases of the RNA sequences we have just have experimental data (as ground truths) in 5 conditions for the first 68 bases. This means we must truncate the output of our model:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQyWGjPHbY33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gru_layer(hidden_dim, dropout):\n",
        "    return tf.keras.layers.Bidirectional(\n",
        "                                tf.keras.layers.GRU(hidden_dim,\n",
        "                                dropout=dropout,\n",
        "                                return_sequences=True,\n",
        "                                kernel_initializer = 'orthogonal'))\n",
        "\n",
        "def lstm_layer(hidden_dim, dropout):\n",
        "    return tf.keras.layers.Bidirectional(\n",
        "                                tf.keras.layers.LSTM(hidden_dim,\n",
        "                                dropout=dropout,\n",
        "                                return_sequences=True,\n",
        "                                kernel_initializer = 'orthogonal'))\n",
        "\n",
        "def build_model(model,seq_len=107, pred_len=68, dropout=0.5,\n",
        "                embed_dim=75, hidden_dim=192, n_sequences=4):\n",
        "    \n",
        "    inputs = tf.keras.layers.Input(shape=(seq_len, n_sequences))\n",
        "\n",
        "    embed = tf.keras.layers.Embedding(input_dim=len(token2int), output_dim=embed_dim)(inputs)\n",
        "    reshaped = tf.reshape(embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n",
        "    \n",
        "    reshaped = tf.keras.layers.SpatialDropout1D(.2)(reshaped)\n",
        "    \n",
        "    if model == 0:\n",
        "        hidden = gru_layer(hidden_dim, dropout)(reshaped)\n",
        "        hidden = gru_layer(hidden_dim, dropout)(hidden)\n",
        "        hidden = gru_layer(hidden_dim, dropout)(hidden)\n",
        "        hidden = gru_layer(hidden_dim, dropout)(hidden)\n",
        "        \n",
        "    if model == 1:\n",
        "        hidden = lstm_layer(hidden_dim, dropout)(reshaped)\n",
        "        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n",
        "        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n",
        "        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n",
        "        \n",
        "    if model == 2:\n",
        "        hidden = lstm_layer(hidden_dim, dropout)(reshaped)\n",
        "        hidden = gru_layer(hidden_dim, dropout)(hidden)\n",
        "        hidden = gru_layer(hidden_dim, dropout)(hidden)\n",
        "        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n",
        "        \n",
        "    if model == 3:\n",
        "        hidden = gru_layer(hidden_dim, dropout)(reshaped)\n",
        "        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n",
        "        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n",
        "        hidden = gru_layer(hidden_dim, dropout)(hidden)\n",
        "    \n",
        "    #only making predictions on the first part of each sequence\n",
        "    truncated = hidden[:, :pred_len]\n",
        "    \n",
        "    out = tf.keras.layers.Dense(5, activation='linear')(truncated)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=out)\n",
        "\n",
        "    #some optimizers\n",
        "    adam = tf.optimizers.Adam()\n",
        "    radam = tfa.optimizers.RectifiedAdam()\n",
        "    lookahead = tfa.optimizers.Lookahead(adam, sync_period=6)\n",
        "    ranger = tfa.optimizers.Lookahead(radam, sync_period=6)\n",
        "    \n",
        "    model.compile(optimizer = adam, loss=MCRMSE)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afuK1X0sbY38",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "\n",
        "**Create train/val split now so both models are trained and evaluated on the same samples:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me2KlwBSbY38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv = KFold(5, shuffle=True, random_state=SEED)\n",
        "folds = {}\n",
        "for i, (train_index, val_index) in enumerate(cv.split(train_inputs)):\n",
        "    folds[i] = val_index\n",
        "    \n",
        "def get_fold(fold, folds):\n",
        "    train_idx = np.concatenate([folds[i] for i in range(len(folds)) if i != fold])\n",
        "    val_idx = folds[fold]\n",
        "    return train_idx, val_idx"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNCelxjFbY3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_inputs, val_inputs, train_labels, val_labels = train_test_split(train_inputs, train_labels, test_size=.01, random_state=34)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69o5ViJxbY4J",
        "colab_type": "text"
      },
      "source": [
        "**We will use a simple learning rate callback for now:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRD4GdmfbY4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_callback = tf.keras.callbacks.ReduceLROnPlateau()\n",
        "#lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=2, mode='min', min_delta=0.00001, cooldown=0, min_lr=1e-8)\n",
        "#lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, verbose=2, mode='min', min_delta=0.001, cooldown=0, min_lr=1e-8)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaWykErkbY4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(model_data, X, y, epochs=150, batch_size=64, verbose=0, patience=13, folds=None):\n",
        "    \n",
        "    paths = []\n",
        "    errors = []\n",
        "    for i in range(len(folds)):\n",
        "        start = time.time()\n",
        "        print('Training fold',i)\n",
        "        train_idx, val_idx = get_fold(i, folds)\n",
        "        \n",
        "        X_train, y_train = X[train_idx], y[train_idx]\n",
        "        X_test, y_test = X[val_idx], y[val_idx]\n",
        "        \n",
        "        gru = build_model(model=model_data[0], n_sequences=X_train.shape[2])\n",
        "        path = model_data[1]+'_fold_'+str(i)+'.h5'\n",
        "        sv_gru = tf.keras.callbacks.ModelCheckpoint(path)\n",
        "        es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=patience)\n",
        "\n",
        "        history = gru.fit(\n",
        "            X_train, y_train, \n",
        "            validation_data=(X_test, y_test),\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            callbacks=[lr_callback, sv_gru, es],\n",
        "            verbose = verbose\n",
        "        )\n",
        "        \n",
        "        best_epoch = np.argmin(history.history['val_loss'])\n",
        "        min_val_loss = history.history['val_loss'][best_epoch]\n",
        "        min_train_loss = history.history['loss'][best_epoch]\n",
        "        paths.append(path)\n",
        "        errors.append(min_val_loss)\n",
        "\n",
        "        print(f\"\\tMin training loss={min_train_loss}, min validation loss={min_val_loss}, elapsed {time.time()-start}\")\n",
        "        \n",
        "    print(f\"\\tOOF CV loss={np.mean(errors)}\")\n",
        "    return list(zip([model_data[0]]*len(paths) , paths)), errors"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TcIWBpdbY4T",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4s8HU0nbY4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_results = []\n",
        "errors = []\n",
        "model_data = [(0,PATH+'model0'), (1,PATH+'model1'), (2,PATH+'model2'), (3,PATH+'model3')]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueUTk5zmbY4a",
        "colab_type": "text"
      },
      "source": [
        "### 1. GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2f2AUbybY4c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "69fbfae6-3a1d-4914-943a-984bbb45b72c"
      },
      "source": [
        "res, error = fit(model_data[0], train_inputs, train_labels, batch_size=16, verbose=2, patience=10, folds=folds)\n",
        "model_results += res\n",
        "errors += error"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training fold 0\n",
            "Epoch 1/150\n",
            "105/105 - 7s - loss: 0.4086 - val_loss: 0.3606\n",
            "Epoch 2/150\n",
            "105/105 - 5s - loss: 0.3585 - val_loss: 0.3548\n",
            "Epoch 3/150\n",
            "105/105 - 5s - loss: 0.3408 - val_loss: 0.3329\n",
            "Epoch 4/150\n",
            "105/105 - 5s - loss: 0.3290 - val_loss: 0.3183\n",
            "Epoch 5/150\n",
            "105/105 - 5s - loss: 0.3162 - val_loss: 0.3072\n",
            "Epoch 6/150\n",
            "105/105 - 6s - loss: 0.3044 - val_loss: 0.2913\n",
            "Epoch 7/150\n",
            "105/105 - 5s - loss: 0.2944 - val_loss: 0.2848\n",
            "Epoch 8/150\n",
            "105/105 - 6s - loss: 0.2858 - val_loss: 0.2721\n",
            "Epoch 9/150\n",
            "105/105 - 5s - loss: 0.2767 - val_loss: 0.2728\n",
            "Epoch 10/150\n",
            "105/105 - 5s - loss: 0.2691 - val_loss: 0.2568\n",
            "Epoch 11/150\n",
            "105/105 - 6s - loss: 0.2631 - val_loss: 0.2510\n",
            "Epoch 12/150\n",
            "105/105 - 5s - loss: 0.2565 - val_loss: 0.2459\n",
            "Epoch 13/150\n",
            "105/105 - 5s - loss: 0.2527 - val_loss: 0.2456\n",
            "Epoch 14/150\n",
            "105/105 - 6s - loss: 0.2498 - val_loss: 0.2418\n",
            "Epoch 15/150\n",
            "105/105 - 5s - loss: 0.2449 - val_loss: 0.2398\n",
            "Epoch 16/150\n",
            "105/105 - 5s - loss: 0.2427 - val_loss: 0.2399\n",
            "Epoch 17/150\n",
            "105/105 - 5s - loss: 0.2386 - val_loss: 0.2363\n",
            "Epoch 18/150\n",
            "105/105 - 6s - loss: 0.2358 - val_loss: 0.2377\n",
            "Epoch 19/150\n",
            "105/105 - 6s - loss: 0.2335 - val_loss: 0.2347\n",
            "Epoch 20/150\n",
            "105/105 - 6s - loss: 0.2306 - val_loss: 0.2308\n",
            "Epoch 21/150\n",
            "105/105 - 5s - loss: 0.2286 - val_loss: 0.2331\n",
            "Epoch 22/150\n",
            "105/105 - 5s - loss: 0.2267 - val_loss: 0.2327\n",
            "Epoch 23/150\n",
            "105/105 - 5s - loss: 0.2238 - val_loss: 0.2319\n",
            "Epoch 24/150\n",
            "105/105 - 5s - loss: 0.2203 - val_loss: 0.2281\n",
            "Epoch 25/150\n",
            "105/105 - 5s - loss: 0.2192 - val_loss: 0.2323\n",
            "Epoch 26/150\n",
            "105/105 - 5s - loss: 0.2176 - val_loss: 0.2268\n",
            "Epoch 27/150\n",
            "105/105 - 5s - loss: 0.2150 - val_loss: 0.2253\n",
            "Epoch 28/150\n",
            "105/105 - 5s - loss: 0.2137 - val_loss: 0.2255\n",
            "Epoch 29/150\n",
            "105/105 - 6s - loss: 0.2124 - val_loss: 0.2250\n",
            "Epoch 30/150\n",
            "105/105 - 6s - loss: 0.2101 - val_loss: 0.2256\n",
            "Epoch 31/150\n",
            "105/105 - 5s - loss: 0.2088 - val_loss: 0.2239\n",
            "Epoch 32/150\n",
            "105/105 - 5s - loss: 0.2066 - val_loss: 0.2277\n",
            "Epoch 33/150\n",
            "105/105 - 6s - loss: 0.2059 - val_loss: 0.2255\n",
            "Epoch 34/150\n",
            "105/105 - 5s - loss: 0.2054 - val_loss: 0.2250\n",
            "Epoch 35/150\n",
            "105/105 - 6s - loss: 0.2025 - val_loss: 0.2262\n",
            "Epoch 36/150\n",
            "105/105 - 6s - loss: 0.2020 - val_loss: 0.2240\n",
            "Epoch 37/150\n",
            "105/105 - 6s - loss: 0.2000 - val_loss: 0.2211\n",
            "Epoch 38/150\n",
            "105/105 - 5s - loss: 0.1992 - val_loss: 0.2262\n",
            "Epoch 39/150\n",
            "105/105 - 5s - loss: 0.1981 - val_loss: 0.2254\n",
            "Epoch 40/150\n",
            "105/105 - 6s - loss: 0.1977 - val_loss: 0.2245\n",
            "Epoch 41/150\n",
            "105/105 - 5s - loss: 0.1965 - val_loss: 0.2258\n",
            "Epoch 42/150\n",
            "105/105 - 5s - loss: 0.1956 - val_loss: 0.2220\n",
            "Epoch 43/150\n",
            "105/105 - 5s - loss: 0.1937 - val_loss: 0.2239\n",
            "Epoch 44/150\n",
            "105/105 - 6s - loss: 0.1929 - val_loss: 0.2232\n",
            "Epoch 45/150\n",
            "105/105 - 5s - loss: 0.1917 - val_loss: 0.2226\n",
            "Epoch 46/150\n",
            "105/105 - 5s - loss: 0.1910 - val_loss: 0.2210\n",
            "Epoch 47/150\n",
            "105/105 - 6s - loss: 0.1904 - val_loss: 0.2201\n",
            "Epoch 48/150\n",
            "105/105 - 6s - loss: 0.1890 - val_loss: 0.2209\n",
            "Epoch 49/150\n",
            "105/105 - 5s - loss: 0.1895 - val_loss: 0.2207\n",
            "Epoch 50/150\n",
            "105/105 - 5s - loss: 0.1887 - val_loss: 0.2197\n",
            "Epoch 51/150\n",
            "105/105 - 5s - loss: 0.1865 - val_loss: 0.2190\n",
            "Epoch 52/150\n",
            "105/105 - 5s - loss: 0.1862 - val_loss: 0.2196\n",
            "Epoch 53/150\n",
            "105/105 - 5s - loss: 0.1855 - val_loss: 0.2213\n",
            "Epoch 54/150\n",
            "105/105 - 6s - loss: 0.1849 - val_loss: 0.2203\n",
            "Epoch 55/150\n",
            "105/105 - 6s - loss: 0.1845 - val_loss: 0.2193\n",
            "Epoch 56/150\n",
            "105/105 - 5s - loss: 0.1836 - val_loss: 0.2194\n",
            "Epoch 57/150\n",
            "105/105 - 5s - loss: 0.1824 - val_loss: 0.2191\n",
            "Epoch 58/150\n",
            "105/105 - 6s - loss: 0.1819 - val_loss: 0.2195\n",
            "Epoch 59/150\n",
            "105/105 - 5s - loss: 0.1820 - val_loss: 0.2192\n",
            "Epoch 60/150\n",
            "105/105 - 5s - loss: 0.1809 - val_loss: 0.2176\n",
            "Epoch 61/150\n",
            "105/105 - 6s - loss: 0.1802 - val_loss: 0.2188\n",
            "Epoch 62/150\n",
            "105/105 - 5s - loss: 0.1794 - val_loss: 0.2188\n",
            "Epoch 63/150\n",
            "105/105 - 6s - loss: 0.1795 - val_loss: 0.2236\n",
            "Epoch 64/150\n",
            "105/105 - 5s - loss: 0.1786 - val_loss: 0.2185\n",
            "Epoch 65/150\n",
            "105/105 - 5s - loss: 0.1780 - val_loss: 0.2202\n",
            "Epoch 66/150\n",
            "105/105 - 5s - loss: 0.1777 - val_loss: 0.2197\n",
            "Epoch 67/150\n",
            "105/105 - 5s - loss: 0.1775 - val_loss: 0.2211\n",
            "Epoch 68/150\n",
            "105/105 - 6s - loss: 0.1764 - val_loss: 0.2184\n",
            "Epoch 69/150\n",
            "105/105 - 6s - loss: 0.1752 - val_loss: 0.2189\n",
            "Epoch 70/150\n",
            "105/105 - 5s - loss: 0.1757 - val_loss: 0.2199\n",
            "\tMin training loss=0.18087823688983917, min validation loss=0.21764183044433594, elapsed 405.98158717155457\n",
            "Training fold 1\n",
            "Epoch 1/150\n",
            "105/105 - 7s - loss: 0.4112 - val_loss: 0.3612\n",
            "Epoch 2/150\n",
            "105/105 - 6s - loss: 0.3570 - val_loss: 0.3316\n",
            "Epoch 3/150\n",
            "105/105 - 6s - loss: 0.3396 - val_loss: 0.3218\n",
            "Epoch 4/150\n",
            "105/105 - 6s - loss: 0.3259 - val_loss: 0.3058\n",
            "Epoch 5/150\n",
            "105/105 - 6s - loss: 0.3145 - val_loss: 0.2956\n",
            "Epoch 6/150\n",
            "105/105 - 6s - loss: 0.3037 - val_loss: 0.2849\n",
            "Epoch 7/150\n",
            "105/105 - 5s - loss: 0.2949 - val_loss: 0.2748\n",
            "Epoch 8/150\n",
            "105/105 - 6s - loss: 0.2848 - val_loss: 0.2654\n",
            "Epoch 9/150\n",
            "105/105 - 5s - loss: 0.2763 - val_loss: 0.2597\n",
            "Epoch 10/150\n",
            "105/105 - 5s - loss: 0.2698 - val_loss: 0.2520\n",
            "Epoch 11/150\n",
            "105/105 - 5s - loss: 0.2642 - val_loss: 0.2474\n",
            "Epoch 12/150\n",
            "105/105 - 6s - loss: 0.2605 - val_loss: 0.2433\n",
            "Epoch 13/150\n",
            "105/105 - 5s - loss: 0.2553 - val_loss: 0.2402\n",
            "Epoch 14/150\n",
            "105/105 - 5s - loss: 0.2509 - val_loss: 0.2390\n",
            "Epoch 15/150\n",
            "105/105 - 6s - loss: 0.2463 - val_loss: 0.2366\n",
            "Epoch 16/150\n",
            "105/105 - 5s - loss: 0.2442 - val_loss: 0.2362\n",
            "Epoch 17/150\n",
            "105/105 - 5s - loss: 0.2399 - val_loss: 0.2339\n",
            "Epoch 18/150\n",
            "105/105 - 6s - loss: 0.2376 - val_loss: 0.2311\n",
            "Epoch 19/150\n",
            "105/105 - 5s - loss: 0.2344 - val_loss: 0.2311\n",
            "Epoch 20/150\n",
            "105/105 - 5s - loss: 0.2326 - val_loss: 0.2292\n",
            "Epoch 21/150\n",
            "105/105 - 5s - loss: 0.2297 - val_loss: 0.2292\n",
            "Epoch 22/150\n",
            "105/105 - 6s - loss: 0.2270 - val_loss: 0.2265\n",
            "Epoch 23/150\n",
            "105/105 - 5s - loss: 0.2254 - val_loss: 0.2294\n",
            "Epoch 24/150\n",
            "105/105 - 6s - loss: 0.2231 - val_loss: 0.2259\n",
            "Epoch 25/150\n",
            "105/105 - 6s - loss: 0.2210 - val_loss: 0.2256\n",
            "Epoch 26/150\n",
            "105/105 - 5s - loss: 0.2190 - val_loss: 0.2221\n",
            "Epoch 27/150\n",
            "105/105 - 6s - loss: 0.2169 - val_loss: 0.2226\n",
            "Epoch 28/150\n",
            "105/105 - 5s - loss: 0.2155 - val_loss: 0.2217\n",
            "Epoch 29/150\n",
            "105/105 - 6s - loss: 0.2130 - val_loss: 0.2225\n",
            "Epoch 30/150\n",
            "105/105 - 6s - loss: 0.2125 - val_loss: 0.2211\n",
            "Epoch 31/150\n",
            "105/105 - 5s - loss: 0.2106 - val_loss: 0.2192\n",
            "Epoch 32/150\n",
            "105/105 - 6s - loss: 0.2084 - val_loss: 0.2192\n",
            "Epoch 33/150\n",
            "105/105 - 5s - loss: 0.2076 - val_loss: 0.2201\n",
            "Epoch 34/150\n",
            "105/105 - 5s - loss: 0.2061 - val_loss: 0.2199\n",
            "Epoch 35/150\n",
            "105/105 - 5s - loss: 0.2041 - val_loss: 0.2183\n",
            "Epoch 36/150\n",
            "105/105 - 5s - loss: 0.2033 - val_loss: 0.2202\n",
            "Epoch 37/150\n",
            "105/105 - 5s - loss: 0.2018 - val_loss: 0.2180\n",
            "Epoch 38/150\n",
            "105/105 - 5s - loss: 0.2007 - val_loss: 0.2185\n",
            "Epoch 39/150\n",
            "105/105 - 5s - loss: 0.1999 - val_loss: 0.2200\n",
            "Epoch 40/150\n",
            "105/105 - 5s - loss: 0.1996 - val_loss: 0.2160\n",
            "Epoch 41/150\n",
            "105/105 - 6s - loss: 0.1976 - val_loss: 0.2159\n",
            "Epoch 42/150\n",
            "105/105 - 6s - loss: 0.1965 - val_loss: 0.2162\n",
            "Epoch 43/150\n",
            "105/105 - 5s - loss: 0.1959 - val_loss: 0.2165\n",
            "Epoch 44/150\n",
            "105/105 - 5s - loss: 0.1949 - val_loss: 0.2169\n",
            "Epoch 45/150\n",
            "105/105 - 6s - loss: 0.1939 - val_loss: 0.2162\n",
            "Epoch 46/150\n",
            "105/105 - 6s - loss: 0.1924 - val_loss: 0.2162\n",
            "Epoch 47/150\n",
            "105/105 - 6s - loss: 0.1927 - val_loss: 0.2138\n",
            "Epoch 48/150\n",
            "105/105 - 6s - loss: 0.1918 - val_loss: 0.2139\n",
            "Epoch 49/150\n",
            "105/105 - 6s - loss: 0.1909 - val_loss: 0.2136\n",
            "Epoch 50/150\n",
            "105/105 - 5s - loss: 0.1897 - val_loss: 0.2185\n",
            "Epoch 51/150\n",
            "105/105 - 5s - loss: 0.1905 - val_loss: 0.2129\n",
            "Epoch 52/150\n",
            "105/105 - 6s - loss: 0.1884 - val_loss: 0.2137\n",
            "Epoch 53/150\n",
            "105/105 - 5s - loss: 0.1872 - val_loss: 0.2131\n",
            "Epoch 54/150\n",
            "105/105 - 6s - loss: 0.1864 - val_loss: 0.2132\n",
            "Epoch 55/150\n",
            "105/105 - 6s - loss: 0.1864 - val_loss: 0.2108\n",
            "Epoch 56/150\n",
            "105/105 - 6s - loss: 0.1854 - val_loss: 0.2126\n",
            "Epoch 57/150\n",
            "105/105 - 6s - loss: 0.1860 - val_loss: 0.2177\n",
            "Epoch 58/150\n",
            "105/105 - 5s - loss: 0.1846 - val_loss: 0.2117\n",
            "Epoch 59/150\n",
            "105/105 - 6s - loss: 0.1833 - val_loss: 0.2135\n",
            "Epoch 60/150\n",
            "105/105 - 6s - loss: 0.1833 - val_loss: 0.2109\n",
            "Epoch 61/150\n",
            "105/105 - 5s - loss: 0.1825 - val_loss: 0.2133\n",
            "Epoch 62/150\n",
            "105/105 - 6s - loss: 0.1821 - val_loss: 0.2127\n",
            "Epoch 63/150\n",
            "105/105 - 5s - loss: 0.1807 - val_loss: 0.2116\n",
            "Epoch 64/150\n",
            "105/105 - 6s - loss: 0.1807 - val_loss: 0.2123\n",
            "Epoch 65/150\n",
            "105/105 - 6s - loss: 0.1806 - val_loss: 0.2122\n",
            "\tMin training loss=0.1863844394683838, min validation loss=0.21077387034893036, elapsed 372.69574189186096\n",
            "Training fold 2\n",
            "Epoch 1/150\n",
            "105/105 - 7s - loss: 0.4120 - val_loss: 0.3661\n",
            "Epoch 2/150\n",
            "105/105 - 5s - loss: 0.3584 - val_loss: 0.3427\n",
            "Epoch 3/150\n",
            "105/105 - 5s - loss: 0.3399 - val_loss: 0.3281\n",
            "Epoch 4/150\n",
            "105/105 - 5s - loss: 0.3265 - val_loss: 0.3112\n",
            "Epoch 5/150\n",
            "105/105 - 5s - loss: 0.3146 - val_loss: 0.3077\n",
            "Epoch 6/150\n",
            "105/105 - 6s - loss: 0.3043 - val_loss: 0.2943\n",
            "Epoch 7/150\n",
            "105/105 - 6s - loss: 0.2922 - val_loss: 0.2824\n",
            "Epoch 8/150\n",
            "105/105 - 5s - loss: 0.2817 - val_loss: 0.2726\n",
            "Epoch 9/150\n",
            "105/105 - 5s - loss: 0.2727 - val_loss: 0.2665\n",
            "Epoch 10/150\n",
            "105/105 - 5s - loss: 0.2651 - val_loss: 0.2588\n",
            "Epoch 11/150\n",
            "105/105 - 5s - loss: 0.2590 - val_loss: 0.2554\n",
            "Epoch 12/150\n",
            "105/105 - 6s - loss: 0.2551 - val_loss: 0.2519\n",
            "Epoch 13/150\n",
            "105/105 - 6s - loss: 0.2494 - val_loss: 0.2485\n",
            "Epoch 14/150\n",
            "105/105 - 5s - loss: 0.2475 - val_loss: 0.2517\n",
            "Epoch 15/150\n",
            "105/105 - 6s - loss: 0.2434 - val_loss: 0.2443\n",
            "Epoch 16/150\n",
            "105/105 - 5s - loss: 0.2389 - val_loss: 0.2434\n",
            "Epoch 17/150\n",
            "105/105 - 5s - loss: 0.2370 - val_loss: 0.2450\n",
            "Epoch 18/150\n",
            "105/105 - 5s - loss: 0.2333 - val_loss: 0.2378\n",
            "Epoch 19/150\n",
            "105/105 - 5s - loss: 0.2323 - val_loss: 0.2424\n",
            "Epoch 20/150\n",
            "105/105 - 6s - loss: 0.2294 - val_loss: 0.2359\n",
            "Epoch 21/150\n",
            "105/105 - 6s - loss: 0.2261 - val_loss: 0.2397\n",
            "Epoch 22/150\n",
            "105/105 - 6s - loss: 0.2239 - val_loss: 0.2347\n",
            "Epoch 23/150\n",
            "105/105 - 5s - loss: 0.2218 - val_loss: 0.2321\n",
            "Epoch 24/150\n",
            "105/105 - 5s - loss: 0.2195 - val_loss: 0.2312\n",
            "Epoch 25/150\n",
            "105/105 - 6s - loss: 0.2176 - val_loss: 0.2338\n",
            "Epoch 26/150\n",
            "105/105 - 6s - loss: 0.2160 - val_loss: 0.2333\n",
            "Epoch 27/150\n",
            "105/105 - 5s - loss: 0.2131 - val_loss: 0.2294\n",
            "Epoch 28/150\n",
            "105/105 - 6s - loss: 0.2128 - val_loss: 0.2316\n",
            "Epoch 29/150\n",
            "105/105 - 6s - loss: 0.2100 - val_loss: 0.2324\n",
            "Epoch 30/150\n",
            "105/105 - 5s - loss: 0.2086 - val_loss: 0.2288\n",
            "Epoch 31/150\n",
            "105/105 - 6s - loss: 0.2066 - val_loss: 0.2265\n",
            "Epoch 32/150\n",
            "105/105 - 6s - loss: 0.2055 - val_loss: 0.2269\n",
            "Epoch 33/150\n",
            "105/105 - 6s - loss: 0.2033 - val_loss: 0.2284\n",
            "Epoch 34/150\n",
            "105/105 - 6s - loss: 0.2032 - val_loss: 0.2264\n",
            "Epoch 35/150\n",
            "105/105 - 6s - loss: 0.2017 - val_loss: 0.2264\n",
            "Epoch 36/150\n",
            "105/105 - 6s - loss: 0.2000 - val_loss: 0.2253\n",
            "Epoch 37/150\n",
            "105/105 - 5s - loss: 0.1987 - val_loss: 0.2254\n",
            "Epoch 38/150\n",
            "105/105 - 6s - loss: 0.1981 - val_loss: 0.2265\n",
            "Epoch 39/150\n",
            "105/105 - 5s - loss: 0.1970 - val_loss: 0.2270\n",
            "Epoch 40/150\n",
            "105/105 - 5s - loss: 0.1960 - val_loss: 0.2261\n",
            "Epoch 41/150\n",
            "105/105 - 6s - loss: 0.1946 - val_loss: 0.2250\n",
            "Epoch 42/150\n",
            "105/105 - 5s - loss: 0.1939 - val_loss: 0.2244\n",
            "Epoch 43/150\n",
            "105/105 - 5s - loss: 0.1923 - val_loss: 0.2265\n",
            "Epoch 44/150\n",
            "105/105 - 6s - loss: 0.1918 - val_loss: 0.2230\n",
            "Epoch 45/150\n",
            "105/105 - 6s - loss: 0.1914 - val_loss: 0.2229\n",
            "Epoch 46/150\n",
            "105/105 - 5s - loss: 0.1903 - val_loss: 0.2233\n",
            "Epoch 47/150\n",
            "105/105 - 6s - loss: 0.1894 - val_loss: 0.2240\n",
            "Epoch 48/150\n",
            "105/105 - 6s - loss: 0.1875 - val_loss: 0.2232\n",
            "Epoch 49/150\n",
            "105/105 - 6s - loss: 0.1876 - val_loss: 0.2237\n",
            "Epoch 50/150\n",
            "105/105 - 5s - loss: 0.1870 - val_loss: 0.2234\n",
            "Epoch 51/150\n",
            "105/105 - 5s - loss: 0.1859 - val_loss: 0.2247\n",
            "Epoch 52/150\n",
            "105/105 - 5s - loss: 0.1850 - val_loss: 0.2227\n",
            "Epoch 53/150\n",
            "105/105 - 6s - loss: 0.1853 - val_loss: 0.2245\n",
            "Epoch 54/150\n",
            "105/105 - 5s - loss: 0.1841 - val_loss: 0.2247\n",
            "Epoch 55/150\n",
            "105/105 - 6s - loss: 0.1834 - val_loss: 0.2237\n",
            "Epoch 56/150\n",
            "105/105 - 5s - loss: 0.1826 - val_loss: 0.2222\n",
            "Epoch 57/150\n",
            "105/105 - 6s - loss: 0.1817 - val_loss: 0.2217\n",
            "Epoch 58/150\n",
            "105/105 - 5s - loss: 0.1813 - val_loss: 0.2214\n",
            "Epoch 59/150\n",
            "105/105 - 6s - loss: 0.1801 - val_loss: 0.2234\n",
            "Epoch 60/150\n",
            "105/105 - 6s - loss: 0.1801 - val_loss: 0.2208\n",
            "Epoch 61/150\n",
            "105/105 - 6s - loss: 0.1794 - val_loss: 0.2216\n",
            "Epoch 62/150\n",
            "105/105 - 6s - loss: 0.1779 - val_loss: 0.2224\n",
            "Epoch 63/150\n",
            "105/105 - 6s - loss: 0.1782 - val_loss: 0.2227\n",
            "Epoch 64/150\n",
            "105/105 - 6s - loss: 0.1778 - val_loss: 0.2200\n",
            "Epoch 65/150\n",
            "105/105 - 5s - loss: 0.1779 - val_loss: 0.2209\n",
            "Epoch 66/150\n",
            "105/105 - 6s - loss: 0.1768 - val_loss: 0.2208\n",
            "Epoch 67/150\n",
            "105/105 - 5s - loss: 0.1759 - val_loss: 0.2205\n",
            "Epoch 68/150\n",
            "105/105 - 5s - loss: 0.1755 - val_loss: 0.2204\n",
            "Epoch 69/150\n",
            "105/105 - 6s - loss: 0.1759 - val_loss: 0.2204\n",
            "Epoch 70/150\n",
            "105/105 - 6s - loss: 0.1744 - val_loss: 0.2199\n",
            "Epoch 71/150\n",
            "105/105 - 6s - loss: 0.1736 - val_loss: 0.2226\n",
            "Epoch 72/150\n",
            "105/105 - 6s - loss: 0.1736 - val_loss: 0.2223\n",
            "Epoch 73/150\n",
            "105/105 - 6s - loss: 0.1739 - val_loss: 0.2221\n",
            "Epoch 74/150\n",
            "105/105 - 5s - loss: 0.1730 - val_loss: 0.2205\n",
            "Epoch 75/150\n",
            "105/105 - 6s - loss: 0.1677 - val_loss: 0.2170\n",
            "Epoch 76/150\n",
            "105/105 - 5s - loss: 0.1654 - val_loss: 0.2173\n",
            "Epoch 77/150\n",
            "105/105 - 5s - loss: 0.1647 - val_loss: 0.2166\n",
            "Epoch 78/150\n",
            "105/105 - 6s - loss: 0.1637 - val_loss: 0.2165\n",
            "Epoch 79/150\n",
            "105/105 - 6s - loss: 0.1634 - val_loss: 0.2168\n",
            "Epoch 80/150\n",
            "105/105 - 5s - loss: 0.1628 - val_loss: 0.2165\n",
            "Epoch 81/150\n",
            "105/105 - 5s - loss: 0.1628 - val_loss: 0.2160\n",
            "Epoch 82/150\n",
            "105/105 - 6s - loss: 0.1623 - val_loss: 0.2161\n",
            "Epoch 83/150\n",
            "105/105 - 6s - loss: 0.1620 - val_loss: 0.2162\n",
            "Epoch 84/150\n",
            "105/105 - 6s - loss: 0.1617 - val_loss: 0.2162\n",
            "Epoch 85/150\n",
            "105/105 - 6s - loss: 0.1616 - val_loss: 0.2160\n",
            "Epoch 86/150\n",
            "105/105 - 5s - loss: 0.1611 - val_loss: 0.2156\n",
            "Epoch 87/150\n",
            "105/105 - 5s - loss: 0.1612 - val_loss: 0.2162\n",
            "Epoch 88/150\n",
            "105/105 - 6s - loss: 0.1607 - val_loss: 0.2160\n",
            "Epoch 89/150\n",
            "105/105 - 5s - loss: 0.1607 - val_loss: 0.2154\n",
            "Epoch 90/150\n",
            "105/105 - 6s - loss: 0.1604 - val_loss: 0.2158\n",
            "Epoch 91/150\n",
            "105/105 - 6s - loss: 0.1604 - val_loss: 0.2159\n",
            "Epoch 92/150\n",
            "105/105 - 6s - loss: 0.1602 - val_loss: 0.2159\n",
            "Epoch 93/150\n",
            "105/105 - 5s - loss: 0.1600 - val_loss: 0.2158\n",
            "Epoch 94/150\n",
            "105/105 - 5s - loss: 0.1599 - val_loss: 0.2163\n",
            "Epoch 95/150\n",
            "105/105 - 6s - loss: 0.1597 - val_loss: 0.2158\n",
            "Epoch 96/150\n",
            "105/105 - 6s - loss: 0.1596 - val_loss: 0.2159\n",
            "Epoch 97/150\n",
            "105/105 - 5s - loss: 0.1592 - val_loss: 0.2159\n",
            "Epoch 98/150\n",
            "105/105 - 5s - loss: 0.1594 - val_loss: 0.2164\n",
            "Epoch 99/150\n",
            "105/105 - 5s - loss: 0.1590 - val_loss: 0.2159\n",
            "\tMin training loss=0.16071248054504395, min validation loss=0.21538414061069489, elapsed 563.926854133606\n",
            "Training fold 3\n",
            "Epoch 1/150\n",
            "105/105 - 8s - loss: 0.4105 - val_loss: 0.3650\n",
            "Epoch 2/150\n",
            "105/105 - 5s - loss: 0.3591 - val_loss: 0.3459\n",
            "Epoch 3/150\n",
            "105/105 - 6s - loss: 0.3437 - val_loss: 0.3274\n",
            "Epoch 4/150\n",
            "105/105 - 6s - loss: 0.3297 - val_loss: 0.3170\n",
            "Epoch 5/150\n",
            "105/105 - 5s - loss: 0.3189 - val_loss: 0.3034\n",
            "Epoch 6/150\n",
            "105/105 - 6s - loss: 0.3048 - val_loss: 0.2921\n",
            "Epoch 7/150\n",
            "105/105 - 6s - loss: 0.2934 - val_loss: 0.2806\n",
            "Epoch 8/150\n",
            "105/105 - 6s - loss: 0.2801 - val_loss: 0.2661\n",
            "Epoch 9/150\n",
            "105/105 - 5s - loss: 0.2724 - val_loss: 0.2606\n",
            "Epoch 10/150\n",
            "105/105 - 6s - loss: 0.2646 - val_loss: 0.2578\n",
            "Epoch 11/150\n",
            "105/105 - 6s - loss: 0.2585 - val_loss: 0.2508\n",
            "Epoch 12/150\n",
            "105/105 - 6s - loss: 0.2555 - val_loss: 0.2502\n",
            "Epoch 13/150\n",
            "105/105 - 6s - loss: 0.2499 - val_loss: 0.2464\n",
            "Epoch 14/150\n",
            "105/105 - 6s - loss: 0.2466 - val_loss: 0.2446\n",
            "Epoch 15/150\n",
            "105/105 - 6s - loss: 0.2424 - val_loss: 0.2439\n",
            "Epoch 16/150\n",
            "105/105 - 6s - loss: 0.2388 - val_loss: 0.2466\n",
            "Epoch 17/150\n",
            "105/105 - 6s - loss: 0.2369 - val_loss: 0.2395\n",
            "Epoch 18/150\n",
            "105/105 - 6s - loss: 0.2332 - val_loss: 0.2373\n",
            "Epoch 19/150\n",
            "105/105 - 6s - loss: 0.2326 - val_loss: 0.2403\n",
            "Epoch 20/150\n",
            "105/105 - 5s - loss: 0.2292 - val_loss: 0.2381\n",
            "Epoch 21/150\n",
            "105/105 - 5s - loss: 0.2254 - val_loss: 0.2411\n",
            "Epoch 22/150\n",
            "105/105 - 6s - loss: 0.2237 - val_loss: 0.2333\n",
            "Epoch 23/150\n",
            "105/105 - 6s - loss: 0.2207 - val_loss: 0.2308\n",
            "Epoch 24/150\n",
            "105/105 - 6s - loss: 0.2191 - val_loss: 0.2323\n",
            "Epoch 25/150\n",
            "105/105 - 6s - loss: 0.2177 - val_loss: 0.2325\n",
            "Epoch 26/150\n",
            "105/105 - 5s - loss: 0.2148 - val_loss: 0.2310\n",
            "Epoch 27/150\n",
            "105/105 - 6s - loss: 0.2131 - val_loss: 0.2286\n",
            "Epoch 28/150\n",
            "105/105 - 6s - loss: 0.2114 - val_loss: 0.2306\n",
            "Epoch 29/150\n",
            "105/105 - 5s - loss: 0.2102 - val_loss: 0.2297\n",
            "Epoch 30/150\n",
            "105/105 - 5s - loss: 0.2085 - val_loss: 0.2294\n",
            "Epoch 31/150\n",
            "105/105 - 6s - loss: 0.2071 - val_loss: 0.2304\n",
            "Epoch 32/150\n",
            "105/105 - 6s - loss: 0.2048 - val_loss: 0.2291\n",
            "Epoch 33/150\n",
            "105/105 - 6s - loss: 0.2033 - val_loss: 0.2278\n",
            "Epoch 34/150\n",
            "105/105 - 6s - loss: 0.2030 - val_loss: 0.2269\n",
            "Epoch 35/150\n",
            "105/105 - 6s - loss: 0.2010 - val_loss: 0.2257\n",
            "Epoch 36/150\n",
            "105/105 - 6s - loss: 0.1997 - val_loss: 0.2263\n",
            "Epoch 37/150\n",
            "105/105 - 6s - loss: 0.1987 - val_loss: 0.2283\n",
            "Epoch 38/150\n",
            "105/105 - 5s - loss: 0.1977 - val_loss: 0.2265\n",
            "Epoch 39/150\n",
            "105/105 - 6s - loss: 0.1960 - val_loss: 0.2257\n",
            "Epoch 40/150\n",
            "105/105 - 6s - loss: 0.1945 - val_loss: 0.2251\n",
            "Epoch 41/150\n",
            "105/105 - 6s - loss: 0.1946 - val_loss: 0.2272\n",
            "Epoch 42/150\n",
            "105/105 - 5s - loss: 0.1927 - val_loss: 0.2249\n",
            "Epoch 43/150\n",
            "105/105 - 6s - loss: 0.1919 - val_loss: 0.2238\n",
            "Epoch 44/150\n",
            "105/105 - 6s - loss: 0.1907 - val_loss: 0.2248\n",
            "Epoch 45/150\n",
            "105/105 - 6s - loss: 0.1901 - val_loss: 0.2249\n",
            "Epoch 46/150\n",
            "105/105 - 6s - loss: 0.1896 - val_loss: 0.2274\n",
            "Epoch 47/150\n",
            "105/105 - 6s - loss: 0.1888 - val_loss: 0.2254\n",
            "Epoch 48/150\n",
            "105/105 - 6s - loss: 0.1886 - val_loss: 0.2233\n",
            "Epoch 49/150\n",
            "105/105 - 6s - loss: 0.1875 - val_loss: 0.2238\n",
            "Epoch 50/150\n",
            "105/105 - 6s - loss: 0.1864 - val_loss: 0.2242\n",
            "Epoch 51/150\n",
            "105/105 - 6s - loss: 0.1853 - val_loss: 0.2239\n",
            "Epoch 52/150\n",
            "105/105 - 6s - loss: 0.1853 - val_loss: 0.2232\n",
            "Epoch 53/150\n",
            "105/105 - 6s - loss: 0.1834 - val_loss: 0.2226\n",
            "Epoch 54/150\n",
            "105/105 - 6s - loss: 0.1834 - val_loss: 0.2241\n",
            "Epoch 55/150\n",
            "105/105 - 6s - loss: 0.1831 - val_loss: 0.2229\n",
            "Epoch 56/150\n",
            "105/105 - 5s - loss: 0.1831 - val_loss: 0.2224\n",
            "Epoch 57/150\n",
            "105/105 - 6s - loss: 0.1819 - val_loss: 0.2226\n",
            "Epoch 58/150\n",
            "105/105 - 6s - loss: 0.1811 - val_loss: 0.2238\n",
            "Epoch 59/150\n",
            "105/105 - 6s - loss: 0.1806 - val_loss: 0.2244\n",
            "Epoch 60/150\n",
            "105/105 - 6s - loss: 0.1796 - val_loss: 0.2223\n",
            "Epoch 61/150\n",
            "105/105 - 6s - loss: 0.1787 - val_loss: 0.2221\n",
            "Epoch 62/150\n",
            "105/105 - 5s - loss: 0.1781 - val_loss: 0.2231\n",
            "Epoch 63/150\n",
            "105/105 - 6s - loss: 0.1779 - val_loss: 0.2221\n",
            "Epoch 64/150\n",
            "105/105 - 6s - loss: 0.1768 - val_loss: 0.2208\n",
            "Epoch 65/150\n",
            "105/105 - 5s - loss: 0.1766 - val_loss: 0.2209\n",
            "Epoch 66/150\n",
            "105/105 - 6s - loss: 0.1764 - val_loss: 0.2212\n",
            "Epoch 67/150\n",
            "105/105 - 5s - loss: 0.1758 - val_loss: 0.2220\n",
            "Epoch 68/150\n",
            "105/105 - 5s - loss: 0.1747 - val_loss: 0.2212\n",
            "Epoch 69/150\n",
            "105/105 - 5s - loss: 0.1748 - val_loss: 0.2223\n",
            "Epoch 70/150\n",
            "105/105 - 6s - loss: 0.1747 - val_loss: 0.2213\n",
            "Epoch 71/150\n",
            "105/105 - 6s - loss: 0.1739 - val_loss: 0.2233\n",
            "Epoch 72/150\n",
            "105/105 - 6s - loss: 0.1730 - val_loss: 0.2206\n",
            "Epoch 73/150\n",
            "105/105 - 6s - loss: 0.1726 - val_loss: 0.2221\n",
            "Epoch 74/150\n",
            "105/105 - 5s - loss: 0.1720 - val_loss: 0.2228\n",
            "Epoch 75/150\n",
            "105/105 - 5s - loss: 0.1718 - val_loss: 0.2210\n",
            "Epoch 76/150\n",
            "105/105 - 5s - loss: 0.1716 - val_loss: 0.2218\n",
            "Epoch 77/150\n",
            "105/105 - 5s - loss: 0.1711 - val_loss: 0.2215\n",
            "Epoch 78/150\n",
            "105/105 - 6s - loss: 0.1704 - val_loss: 0.2214\n",
            "Epoch 79/150\n",
            "105/105 - 6s - loss: 0.1700 - val_loss: 0.2215\n",
            "Epoch 80/150\n",
            "105/105 - 5s - loss: 0.1701 - val_loss: 0.2217\n",
            "Epoch 81/150\n",
            "105/105 - 5s - loss: 0.1687 - val_loss: 0.2237\n",
            "Epoch 82/150\n",
            "105/105 - 6s - loss: 0.1686 - val_loss: 0.2247\n",
            "\tMin training loss=0.17301581799983978, min validation loss=0.2205904871225357, elapsed 472.61035418510437\n",
            "Training fold 4\n",
            "Epoch 1/150\n",
            "105/105 - 8s - loss: 0.4116 - val_loss: 0.3582\n",
            "Epoch 2/150\n",
            "105/105 - 5s - loss: 0.3612 - val_loss: 0.3375\n",
            "Epoch 3/150\n",
            "105/105 - 6s - loss: 0.3427 - val_loss: 0.3234\n",
            "Epoch 4/150\n",
            "105/105 - 6s - loss: 0.3315 - val_loss: 0.3116\n",
            "Epoch 5/150\n",
            "105/105 - 5s - loss: 0.3182 - val_loss: 0.2917\n",
            "Epoch 6/150\n",
            "105/105 - 6s - loss: 0.3047 - val_loss: 0.2831\n",
            "Epoch 7/150\n",
            "105/105 - 5s - loss: 0.2936 - val_loss: 0.2707\n",
            "Epoch 8/150\n",
            "105/105 - 5s - loss: 0.2838 - val_loss: 0.2631\n",
            "Epoch 9/150\n",
            "105/105 - 6s - loss: 0.2753 - val_loss: 0.2578\n",
            "Epoch 10/150\n",
            "105/105 - 5s - loss: 0.2694 - val_loss: 0.2526\n",
            "Epoch 11/150\n",
            "105/105 - 5s - loss: 0.2621 - val_loss: 0.2466\n",
            "Epoch 12/150\n",
            "105/105 - 5s - loss: 0.2575 - val_loss: 0.2458\n",
            "Epoch 13/150\n",
            "105/105 - 6s - loss: 0.2546 - val_loss: 0.2413\n",
            "Epoch 14/150\n",
            "105/105 - 6s - loss: 0.2501 - val_loss: 0.2449\n",
            "Epoch 15/150\n",
            "105/105 - 6s - loss: 0.2478 - val_loss: 0.2342\n",
            "Epoch 16/150\n",
            "105/105 - 6s - loss: 0.2433 - val_loss: 0.2400\n",
            "Epoch 17/150\n",
            "105/105 - 6s - loss: 0.2406 - val_loss: 0.2379\n",
            "Epoch 18/150\n",
            "105/105 - 6s - loss: 0.2366 - val_loss: 0.2337\n",
            "Epoch 19/150\n",
            "105/105 - 6s - loss: 0.2346 - val_loss: 0.2324\n",
            "Epoch 20/150\n",
            "105/105 - 6s - loss: 0.2325 - val_loss: 0.2360\n",
            "Epoch 21/150\n",
            "105/105 - 5s - loss: 0.2297 - val_loss: 0.2309\n",
            "Epoch 22/150\n",
            "105/105 - 5s - loss: 0.2267 - val_loss: 0.2271\n",
            "Epoch 23/150\n",
            "105/105 - 5s - loss: 0.2257 - val_loss: 0.2281\n",
            "Epoch 24/150\n",
            "105/105 - 6s - loss: 0.2226 - val_loss: 0.2287\n",
            "Epoch 25/150\n",
            "105/105 - 6s - loss: 0.2204 - val_loss: 0.2309\n",
            "Epoch 26/150\n",
            "105/105 - 6s - loss: 0.2185 - val_loss: 0.2262\n",
            "Epoch 27/150\n",
            "105/105 - 5s - loss: 0.2162 - val_loss: 0.2231\n",
            "Epoch 28/150\n",
            "105/105 - 6s - loss: 0.2147 - val_loss: 0.2244\n",
            "Epoch 29/150\n",
            "105/105 - 6s - loss: 0.2135 - val_loss: 0.2234\n",
            "Epoch 30/150\n",
            "105/105 - 6s - loss: 0.2109 - val_loss: 0.2227\n",
            "Epoch 31/150\n",
            "105/105 - 6s - loss: 0.2095 - val_loss: 0.2229\n",
            "Epoch 32/150\n",
            "105/105 - 5s - loss: 0.2080 - val_loss: 0.2224\n",
            "Epoch 33/150\n",
            "105/105 - 6s - loss: 0.2071 - val_loss: 0.2270\n",
            "Epoch 34/150\n",
            "105/105 - 6s - loss: 0.2055 - val_loss: 0.2244\n",
            "Epoch 35/150\n",
            "105/105 - 6s - loss: 0.2039 - val_loss: 0.2199\n",
            "Epoch 36/150\n",
            "105/105 - 5s - loss: 0.2036 - val_loss: 0.2217\n",
            "Epoch 37/150\n",
            "105/105 - 6s - loss: 0.2013 - val_loss: 0.2214\n",
            "Epoch 38/150\n",
            "105/105 - 5s - loss: 0.2003 - val_loss: 0.2180\n",
            "Epoch 39/150\n",
            "105/105 - 6s - loss: 0.1981 - val_loss: 0.2192\n",
            "Epoch 40/150\n",
            "105/105 - 5s - loss: 0.1973 - val_loss: 0.2172\n",
            "Epoch 41/150\n",
            "105/105 - 6s - loss: 0.1966 - val_loss: 0.2194\n",
            "Epoch 42/150\n",
            "105/105 - 5s - loss: 0.1961 - val_loss: 0.2193\n",
            "Epoch 43/150\n",
            "105/105 - 6s - loss: 0.1955 - val_loss: 0.2173\n",
            "Epoch 44/150\n",
            "105/105 - 6s - loss: 0.1943 - val_loss: 0.2213\n",
            "Epoch 45/150\n",
            "105/105 - 5s - loss: 0.1931 - val_loss: 0.2188\n",
            "Epoch 46/150\n",
            "105/105 - 6s - loss: 0.1929 - val_loss: 0.2203\n",
            "Epoch 47/150\n",
            "105/105 - 6s - loss: 0.1909 - val_loss: 0.2170\n",
            "Epoch 48/150\n",
            "105/105 - 5s - loss: 0.1902 - val_loss: 0.2190\n",
            "Epoch 49/150\n",
            "105/105 - 6s - loss: 0.1891 - val_loss: 0.2197\n",
            "Epoch 50/150\n",
            "105/105 - 5s - loss: 0.1887 - val_loss: 0.2169\n",
            "Epoch 51/150\n",
            "105/105 - 5s - loss: 0.1878 - val_loss: 0.2190\n",
            "Epoch 52/150\n",
            "105/105 - 6s - loss: 0.1868 - val_loss: 0.2198\n",
            "Epoch 53/150\n",
            "105/105 - 6s - loss: 0.1860 - val_loss: 0.2189\n",
            "Epoch 54/150\n",
            "105/105 - 6s - loss: 0.1858 - val_loss: 0.2198\n",
            "Epoch 55/150\n",
            "105/105 - 5s - loss: 0.1854 - val_loss: 0.2207\n",
            "Epoch 56/150\n",
            "105/105 - 6s - loss: 0.1853 - val_loss: 0.2189\n",
            "Epoch 57/150\n",
            "105/105 - 6s - loss: 0.1842 - val_loss: 0.2159\n",
            "Epoch 58/150\n",
            "105/105 - 6s - loss: 0.1830 - val_loss: 0.2172\n",
            "Epoch 59/150\n",
            "105/105 - 6s - loss: 0.1825 - val_loss: 0.2195\n",
            "Epoch 60/150\n",
            "105/105 - 5s - loss: 0.1819 - val_loss: 0.2177\n",
            "Epoch 61/150\n",
            "105/105 - 6s - loss: 0.1821 - val_loss: 0.2181\n",
            "Epoch 62/150\n",
            "105/105 - 6s - loss: 0.1808 - val_loss: 0.2186\n",
            "Epoch 63/150\n",
            "105/105 - 6s - loss: 0.1796 - val_loss: 0.2198\n",
            "Epoch 64/150\n",
            "105/105 - 6s - loss: 0.1790 - val_loss: 0.2170\n",
            "Epoch 65/150\n",
            "105/105 - 5s - loss: 0.1791 - val_loss: 0.2174\n",
            "Epoch 66/150\n",
            "105/105 - 5s - loss: 0.1783 - val_loss: 0.2192\n",
            "Epoch 67/150\n",
            "105/105 - 6s - loss: 0.1779 - val_loss: 0.2156\n",
            "Epoch 68/150\n",
            "105/105 - 6s - loss: 0.1771 - val_loss: 0.2168\n",
            "Epoch 69/150\n",
            "105/105 - 5s - loss: 0.1768 - val_loss: 0.2175\n",
            "Epoch 70/150\n",
            "105/105 - 6s - loss: 0.1766 - val_loss: 0.2155\n",
            "Epoch 71/150\n",
            "105/105 - 5s - loss: 0.1762 - val_loss: 0.2160\n",
            "Epoch 72/150\n",
            "105/105 - 6s - loss: 0.1758 - val_loss: 0.2189\n",
            "Epoch 73/150\n",
            "105/105 - 5s - loss: 0.1745 - val_loss: 0.2155\n",
            "Epoch 74/150\n",
            "105/105 - 6s - loss: 0.1744 - val_loss: 0.2164\n",
            "Epoch 75/150\n",
            "105/105 - 6s - loss: 0.1734 - val_loss: 0.2162\n",
            "Epoch 76/150\n",
            "105/105 - 6s - loss: 0.1730 - val_loss: 0.2155\n",
            "Epoch 77/150\n",
            "105/105 - 6s - loss: 0.1731 - val_loss: 0.2169\n",
            "Epoch 78/150\n",
            "105/105 - 5s - loss: 0.1727 - val_loss: 0.2173\n",
            "Epoch 79/150\n",
            "105/105 - 5s - loss: 0.1720 - val_loss: 0.2187\n",
            "Epoch 80/150\n",
            "105/105 - 5s - loss: 0.1722 - val_loss: 0.2173\n",
            "Epoch 81/150\n",
            "105/105 - 5s - loss: 0.1665 - val_loss: 0.2131\n",
            "Epoch 82/150\n",
            "105/105 - 5s - loss: 0.1643 - val_loss: 0.2125\n",
            "Epoch 83/150\n",
            "105/105 - 6s - loss: 0.1637 - val_loss: 0.2127\n",
            "Epoch 84/150\n",
            "105/105 - 6s - loss: 0.1629 - val_loss: 0.2124\n",
            "Epoch 85/150\n",
            "105/105 - 6s - loss: 0.1624 - val_loss: 0.2123\n",
            "Epoch 86/150\n",
            "105/105 - 6s - loss: 0.1618 - val_loss: 0.2117\n",
            "Epoch 87/150\n",
            "105/105 - 5s - loss: 0.1616 - val_loss: 0.2120\n",
            "Epoch 88/150\n",
            "105/105 - 5s - loss: 0.1614 - val_loss: 0.2125\n",
            "Epoch 89/150\n",
            "105/105 - 5s - loss: 0.1610 - val_loss: 0.2122\n",
            "Epoch 90/150\n",
            "105/105 - 6s - loss: 0.1605 - val_loss: 0.2120\n",
            "Epoch 91/150\n",
            "105/105 - 6s - loss: 0.1601 - val_loss: 0.2123\n",
            "Epoch 92/150\n",
            "105/105 - 5s - loss: 0.1602 - val_loss: 0.2120\n",
            "Epoch 93/150\n",
            "105/105 - 6s - loss: 0.1599 - val_loss: 0.2120\n",
            "Epoch 94/150\n",
            "105/105 - 5s - loss: 0.1600 - val_loss: 0.2120\n",
            "Epoch 95/150\n",
            "105/105 - 6s - loss: 0.1598 - val_loss: 0.2120\n",
            "Epoch 96/150\n",
            "105/105 - 6s - loss: 0.1592 - val_loss: 0.2119\n",
            "\tMin training loss=0.16184845566749573, min validation loss=0.21165695786476135, elapsed 548.3694179058075\n",
            "\tOOF CV loss=0.21520945727825164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG9KeUClbY4g",
        "colab_type": "text"
      },
      "source": [
        "### 2. LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peQROjFTbY4g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "758f6fc3-84a9-494e-bc84-ba49c324a2d7"
      },
      "source": [
        "res, error = fit(model_data[1], train_inputs, train_labels, batch_size=16,verbose=0, patience=10, folds=folds)\n",
        "model_results += res\n",
        "errors += error"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training fold 0\n",
            "\tMin training loss=0.16203252971172333, min validation loss=0.21419930458068848, elapsed 420.7448251247406\n",
            "Training fold 1\n",
            "\tMin training loss=0.17029674351215363, min validation loss=0.20865190029144287, elapsed 379.589262008667\n",
            "Training fold 2\n",
            "\tMin training loss=0.17418727278709412, min validation loss=0.2194335162639618, elapsed 334.0083656311035\n",
            "Training fold 3\n",
            "\tMin training loss=0.15271352231502533, min validation loss=0.2181905210018158, elapsed 489.6445505619049\n",
            "Training fold 4\n",
            "\tMin training loss=0.15969733893871307, min validation loss=0.21242062747478485, elapsed 447.50039052963257\n",
            "\tOOF CV loss=0.21457917392253875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHoX0QQWbY4k",
        "colab_type": "text"
      },
      "source": [
        "### 3. LSTM + GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdkXXkQXbY4l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "e80dbadd-d04e-4201-a267-bb24c2231507"
      },
      "source": [
        "res, error = fit(model_data[2], train_inputs, train_labels, batch_size=16,verbose=0, folds=folds)\n",
        "model_results += res\n",
        "errors += error"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training fold 0\n",
            "\tMin training loss=0.15079553425312042, min validation loss=0.2133994847536087, elapsed 685.0200002193451\n",
            "Training fold 1\n",
            "\tMin training loss=0.16434916853904724, min validation loss=0.20793390274047852, elapsed 483.7919330596924\n",
            "Training fold 2\n",
            "\tMin training loss=0.15954819321632385, min validation loss=0.214729443192482, elapsed 662.0508465766907\n",
            "Training fold 3\n",
            "\tMin training loss=0.1562618464231491, min validation loss=0.2181054949760437, elapsed 633.3748867511749\n",
            "Training fold 4\n",
            "\tMin training loss=0.15845908224582672, min validation loss=0.21068955957889557, elapsed 714.9008951187134\n",
            "\tOOF CV loss=0.2129715770483017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCMv_nf6bY4o",
        "colab_type": "text"
      },
      "source": [
        "### 4. GRU + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLrLj8uDbY4p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "dc7944fa-5992-4746-b899-545414604673"
      },
      "source": [
        "res, error = fit(model_data[3], train_inputs, train_labels, batch_size=16,verbose=0, folds=folds)\n",
        "model_results += res\n",
        "errors += error"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training fold 0\n",
            "\tMin training loss=0.15797114372253418, min validation loss=0.21113689243793488, elapsed 493.18807888031006\n",
            "Training fold 1\n",
            "\tMin training loss=0.15805599093437195, min validation loss=0.20538043975830078, elapsed 488.47091031074524\n",
            "Training fold 2\n",
            "\tMin training loss=0.15113264322280884, min validation loss=0.21359628438949585, elapsed 662.4131855964661\n",
            "Training fold 3\n",
            "\tMin training loss=0.1477593183517456, min validation loss=0.21534012258052826, elapsed 617.0750877857208\n",
            "Training fold 4\n",
            "\tMin training loss=0.15797513723373413, min validation loss=0.20937970280647278, elapsed 496.25954246520996\n",
            "\tOOF CV loss=0.21096668839454652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6tZceHbbY4t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d76e5f7c-0712-4e45-f8c5-f0be685d1736"
      },
      "source": [
        "print('CV error is ', np.mean(errors))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV error is  0.21343172416090966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPT2Y3zZbY4x",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTvC-FhrbY4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public_df = test.query(\"seq_length == 107\").copy()\n",
        "private_df = test.query(\"seq_length == 130\").copy()\n",
        "\n",
        "public_inputs = preprocess_inputs(public_df, cols=['sequence', 'structure', 'predicted_loop_type', 'pairs'], map_tokens=token2int)\n",
        "private_inputs = preprocess_inputs(private_df, cols=['sequence', 'structure', 'predicted_loop_type', 'pairs'], map_tokens=token2int)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck3ta8scbY40",
        "colab_type": "text"
      },
      "source": [
        "**Predict twice, one for the public leaderboard, the other for the private leaderboard:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIoj_2dbbY40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_trained_models(model, path):\n",
        "    short = build_model(model, seq_len=107, pred_len=107)\n",
        "    short.load_weights(path)\n",
        "    long =  build_model(model, seq_len=130, pred_len=130)\n",
        "    long.load_weights(path)\n",
        "    return predict_pair(short, long)\n",
        "    #return short, long\n",
        "\n",
        "def predict_pair(short, long):\n",
        "    public = short.predict(public_inputs)\n",
        "    private = long.predict(private_inputs)\n",
        "    return public, private"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgO2qfQibY45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_n(f):\n",
        "    if f[6:14]=='gru_lstm': return 3\n",
        "    if f[6:14]=='lstm_gru': return 2\n",
        "    if f[6:10]=='lstm': return 1\n",
        "    if f[6:9]=='gru': return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8VuBWc_bY5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from glob import glob\n",
        "#model_results = glob('*.h5')\n",
        "#model_results = [( get_n(f), f) for f in model_results]\n",
        "#model_results = [(n, f) for n,f in model_results if n==3]\n",
        "#model_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCOiMJIjbY5F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4985b020-6d4c-4c4f-f909-5f54ede2c7c9"
      },
      "source": [
        "#build all models\n",
        "#models = [load_trained_models(m,p) for m,p in tqdm(model_results) ]\n",
        "\n",
        "#and predict\n",
        "#predictions = [predict_pair(short, long) for short, long in tqdm(models)]\n",
        "predictions = [load_trained_models(m,p) for m,p in tqdm(model_results) ]\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [03:49<00:00, 11.46s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx7Ua21ibY5I",
        "colab_type": "text"
      },
      "source": [
        "# Postprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QwODr8bbY5I",
        "colab_type": "text"
      },
      "source": [
        "### Adjust Output\n",
        "According to pubic and private test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoP3-J95bY5L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c903076-dd91-4fd1-92e2-a25e2b93f7b1"
      },
      "source": [
        "dfs  = [adjust(public_df, private_df, public_preds, private_preds) for public_preds, private_preds in tqdm(predictions)]\n",
        "weights = [1/len(dfs)] * len(dfs)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [01:10<00:00,  3.55s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYhoCmz3bY5O",
        "colab_type": "text"
      },
      "source": [
        "### Blending"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YezOwMgbY5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "blend_preds_df = pd.DataFrame({'id_seqpos':dfs[0]['id_seqpos']})\n",
        "for c in target_cols:\n",
        "    blend_preds_df[c] = blend_column(dfs, weights, c)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpwoD5FzbY5V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "a920570f-5b16-4b18-b04d-b1a3dc1cfcac"
      },
      "source": [
        "submission = sample_sub[['id_seqpos']].merge(blend_preds_df, on=['id_seqpos'])\n",
        "\n",
        "#sanity check\n",
        "submission.head()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_seqpos</th>\n",
              "      <th>reactivity</th>\n",
              "      <th>deg_Mg_pH10</th>\n",
              "      <th>deg_pH10</th>\n",
              "      <th>deg_Mg_50C</th>\n",
              "      <th>deg_50C</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_00073f8be_0</td>\n",
              "      <td>0.746345</td>\n",
              "      <td>0.649274</td>\n",
              "      <td>1.960138</td>\n",
              "      <td>0.547165</td>\n",
              "      <td>0.765299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_00073f8be_1</td>\n",
              "      <td>2.223649</td>\n",
              "      <td>3.185862</td>\n",
              "      <td>3.910727</td>\n",
              "      <td>3.246317</td>\n",
              "      <td>2.777526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_00073f8be_2</td>\n",
              "      <td>1.573120</td>\n",
              "      <td>0.615004</td>\n",
              "      <td>0.661253</td>\n",
              "      <td>0.725471</td>\n",
              "      <td>0.678785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_00073f8be_3</td>\n",
              "      <td>1.317544</td>\n",
              "      <td>1.148795</td>\n",
              "      <td>1.186439</td>\n",
              "      <td>1.658210</td>\n",
              "      <td>1.869232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_00073f8be_4</td>\n",
              "      <td>0.821595</td>\n",
              "      <td>0.597514</td>\n",
              "      <td>0.543067</td>\n",
              "      <td>0.859756</td>\n",
              "      <td>0.913399</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id_seqpos  reactivity  deg_Mg_pH10  deg_pH10  deg_Mg_50C   deg_50C\n",
              "0  id_00073f8be_0    0.746345     0.649274  1.960138    0.547165  0.765299\n",
              "1  id_00073f8be_1    2.223649     3.185862  3.910727    3.246317  2.777526\n",
              "2  id_00073f8be_2    1.573120     0.615004  0.661253    0.725471  0.678785\n",
              "3  id_00073f8be_3    1.317544     1.148795  1.186439    1.658210  1.869232\n",
              "4  id_00073f8be_4    0.821595     0.597514  0.543067    0.859756  0.913399"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ly9NBuDbY5Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c426d970-a73f-4193-a5b1-f56ebf8b071f"
      },
      "source": [
        "submission.to_csv(ROOT+PATH+'submission.csv', index=False)\n",
        "print('Submission saved')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Submission saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL56VsdvbY5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}